# server/translate/openai_translate.py
import aiohttp
from ..ai_translator.ai_translator import initialize_config

_LANG_NAME = {
    "zh": "ä¸­æ–‡", "zh_CN": "ä¸­æ–‡", "zh_TW": "ä¸­æ–‡ï¼ˆç¹ä½“ï¼‰",
    "en": "è‹±æ–‡", "ja": "æ—¥æ–‡", "ko": "éŸ©æ–‡",
    "fr": "æ³•è¯­", "de": "å¾·è¯­", "es": "è¥¿ç­ç‰™è¯­",
    "ru": "ä¿„è¯­", "it": "æ„å¤§åˆ©è¯­", "pt": "è‘¡è„ç‰™è¯­"
}


def _lang_to_name(code: str) -> str:
    if not code:
        return "ä¸­æ–‡"
    return _LANG_NAME.get(code, code)


async def openai_translate(text: str, target_lang_code: str) -> str:
    cfg = initialize_config()
    api_key = cfg.get("api_key", "")
    base_url = cfg.get("base_url", "https://api.openai.com/v1").rstrip("/")
    model = cfg.get("model", "gpt-4o-mini")

    if not api_key:
        raise RuntimeError("OpenAI api_key æœªé…ç½®")

    url = f"{base_url}/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": model,
        "messages": [
            {
                "role": "system",
                "content": f"Translate 'text' field to {_lang_to_name(target_lang_code)} in JSON. If word has NSFW meaning, prioritize NSFW interpretation. Return in JSON with no changes."
            },
            {"role": "user", "content": text}
        ],
        "response_format": {"type": "json_object"},
        "temperature": 0
    }

    # é…ç½®ä¼šè¯ä»¥ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­çš„ä»£ç†è®¾ç½®
    async with aiohttp.ClientSession(trust_env=True) as session:
        async with session.post(url, headers=headers, json=payload, timeout=60) as resp:
            data = await resp.json()
            if resp.status != 200:
                raise RuntimeError(f"OpenAI æ¥å£é”™è¯¯: {resp.status} {data}")

            result = data["choices"][0]["message"]["content"].strip()
            usage = data.get("usage", {})
            prompt_tokens = usage.get("prompt_tokens", 0)
            completion_tokens = usage.get("completion_tokens", 0)
            total_tokens = usage.get(
                "total_tokens", prompt_tokens + completion_tokens)

            # æ§åˆ¶å°æ—¥å¿—
            # print(f"ğŸ¤– æ­£åœ¨ä½¿ç”¨OpenAIç¿»è¯‘: {text}")
            print(f"ğŸ¤– æ­£åœ¨ä½¿ç”¨OpenAIç¿»è¯‘")
            print(
                f"ğŸ“Š OpenAIç¿»è¯‘tokensä½¿ç”¨: {prompt_tokens}+{completion_tokens}={total_tokens}")
            # print(f"âœ… OpenAIç¿»è¯‘æˆåŠŸ: {text} -> {result}")

            return result
